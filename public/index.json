[{"content":"Proxmox is my new virtualization platform of choice for running containerized workloads at home. After several detours through configuration rabbit holes, here\u0026rsquo;s what I learned setting up LXC containers for Dokploy and other services.\nChoosing the Right OS for Dokploy Debian 12 didn\u0026rsquo;t cooperate. Missing curl dependencies and repository connection issues made me switch quickly. Ubuntu 14.4 worked smoothly‚Äîcurl installed cleanly and Dokploy deployed with a single command.\nMy initial LXC container specs were generous, probably too generous:\n8 cores 8GB memory 64GB storage 512MB swap I\u0026rsquo;ll scale these down after monitoring actual usage.\nThe IP Address problem Here\u0026rsquo;s where I lost hours: Proxmox saves the initial DHCP-assigned IP as static in /etc/network/interfaces. When I changed the container to a different static IP, the Opnsense router kept showing the old address.\nThe fix was editing Proxmox\u0026rsquo;s network config:\nnano /etc/network/interfaces # Change this line from static to dhcp iface vmbr0 inet static I repeated this exact mistake with Docker Swarm. Changing the host IP after swarm initialization broke everything. A forced docker swarm leave -f and reinit disconnected Dokploy completely. I ended up reinstalling Dokploy rather than debugging further.\nPro tip: decide your IP addressing scheme before deploying Dokploy. Save yourself the headache.\nEssential Proxmox Configuration After installation:\nDisable enterprise repositories Add no-subscription repos Update package lists Reboot (unfortunately necessary) Keep unprivileged containers enabled (the default). This prevents containers from accessing the entire host system.\nFinding Container Ports New applications deployed through Dokploy are accessible via http://\u0026lt;host-ip\u0026gt;:\u0026lt;port\u0026gt;. The port mapping isn\u0026rsquo;t obvious from Docker files.\nNavigate to: Dashboard ‚Üí Docker tab ‚Üí Row actions (three dots) ‚Üí View config ‚Üí Search for \u0026ldquo;HostPort\u0026rdquo;\nThat three-dot menu hides surprisingly well, if you have a small screen .\nUSB Passthrough for Zigbee If you are using Homeassistant, getting home automation hardware connected is straightforward:\nlsusb # Check what\u0026#39;s available ls /dev/serial/by-id/ # List USB devices In Proxmox check: Container ‚Üí Resources ‚Üí Add ‚Üí Device passthrough My Zigbee dongle path for example is:\n/dev/serial/by-id/usb-dresden_elektronik_ingenieurtechnik_GmbH_ConBee_II_DE2478649-if00 If the usb is recognized, you need to add it to your LXC container (in this case Dokploy) from the proxmox interface.\nSome changes required in the Homeassistant compose file inside Dokploy:\nrestart: in case the host machine / dockploy is restarted devices: manual mapping of usb from dokploy LXC container to HA container group_add: adds Homeassistant user to root group, for usb access permission services: homeassistant: image: ghcr.io/home-assistant/home-assistant:stable restart: unless-stopped devices: - /dev/serial/by-id/\u0026lt;usb_file_name\u0026gt;:/dev/serial/by-id/\u0026lt;usb_file_name\u0026gt; group_add: - \u0026#34;0\u0026#34; LXC Config Files Container configurations live at /etc/pve/lxc/. Not sure when I\u0026rsquo;ll need direct access to these, but it\u0026rsquo;s good to know in case I want to manually test a setting.\nHelper Scripts and Community Templates The Proxmox Community Scripts project offers quick setups for various services. I prefer following official installation instructions, though. For Dokploy, I created a basic Ubuntu LXC, installed curl, and ran their one-line installer.\nAlways review scripts before running them. At minimum, have an AI analyze it if you\u0026rsquo;re short on time.\nGetting Back to Building Apps Facepalm moment: I spent hours troubleshooting that IP change when I could have been building applications. All this because I changed from dynamic to static IP mid-deployment. Next time I\u0026rsquo;ll do the planning upfront.\nDocker Swarm through Dokploy makes container management simple. I deployed Convex DB with its dashboard in minutes. The built-in metrics need zero configuration‚Äîwatching resource usage without manual Prometheus setups feels refreshing.\nPortainer runs alongside Dokploy for additional statistics and container management options. Both installed in under five minutes.\nProxmox rewards experimentation. Each configuration teaches something new about networking, containers, or Linux systems. My next challenge: backup strategies and network storage.\nWhat\u0026rsquo;s your home lab setup? Share your Proxmox experiences or container orchestration choices. I\u0026rsquo;m curious how others approach the storage puzzle.\n","permalink":"https://liviuiancu.com/posts/proxmox-for-dokploy/","summary":"Proxmox is my new virtualization platform of choice for running containerized workloads at home. After several detours through configuration rabbit holes, here\u0026rsquo;s what I learned setting up LXC containers for Dokploy and other services.","title":"Running Proxmox and Dokploy for my home lab"},{"content":"Self-hosting modern application backends is often a puzzle of configuration, especially when working with newer architectures and tools. Convex, with its focus on full-stack development, offers a self-hosted option that\u0026rsquo;s perfect for my home lab environment.\nThis article details the key adjustments I made to successfully run the Convex DB, dashboard, and a basic chat application frontend using Docker, specifically testing on an Orbstack setup on a MacOS Mini M4 ARM machine.\nüõ†Ô∏è The Self-Hosting Setup The foundation of this setup follows the official Convex self-hosted instructions but required crucial configuration changes to ensure the containers could correctly communicate across the Docker network, especially in the context of Orbstack\u0026rsquo;s local hostname resolution.\n1. Frontend Container Configuration The frontend (FE) application needs to know where to find the self-hosted Convex backend, and it must also listen on an accessible address within the container.\nConvex Self-Hosted URL: In your Convex application\u0026rsquo;s .env.local, specify the hostname resolvable by the FE container. In an Orbstack environment, this often looks like an internal .orb.local address:\nCONVEX_SELF_HOSTED_URL=https://\u0026lt;providedby\u0026gt;.orb.local CONVEX_SELF_HOSTED_ADMIN_KEY=your_admin_key Insight: Using a specific hostname instead of localhost or 127.0.0.1 ensures that the connection resolves correctly from inside the frontend container to the host or the dedicated Convex backend service.\nFrontend Server Binding: For the container\u0026rsquo;s port mapping to work, the development server must listen on all available network interfaces (0.0.0.0) inside the container, not just localhost.\npackage.json (e.g., for a Vite app): \u0026#34;dev:frontend\u0026#34;: \u0026#34;vite --open --host\u0026#34; vite.config.mts (Crucial for host access): export default defineConfig({ // ... server: { host: true, // Listen on 0.0.0.0 allowedHosts: [ \u0026#39;\u0026lt;frontend-container-url\u0026gt;.orb.local\u0026#39;, // Allow host access ] } }); The above changes successfully run a basic chat app.\nPort Mapping: Expose the frontend port from the container to your host machine.\nports: - \u0026#34;5173:5173\u0026#34; 2. Convex DB Container Configuration The Convex DB and Convex Dashboard containers are managed via docker-compose. Network configuration and environment variables were the primary points of adjustment.\nShared Network: The FE and DB containers must share the same Docker network to communicate seamlessly. I used an already created bridge network named echoapp:\n# compose.yaml ... networks: - convex-net networks: convex-net: external: true name: echoapp # Needs to match the FE container\u0026#39;s network driver: bridge Origin Environment Variables: These variables dictate how Convex generates URLs internally for various services. All must point to the resolvable host address.\ncompose.yaml (Orbstack Links): - CONVEX_CLOUD_ORIGIN=https://\u0026lt;providedby\u0026gt;.orb.local - CONVEX_SITE_ORIGIN=https://\u0026lt;providedby\u0026gt;.orb.local .env file for convex docker compose: CONVEX_CLOUD_ORIGIN=\u0026#39;https://\u0026lt;providedby\u0026gt;.orb.local\u0026#39; CONVEX_SITE_ORIGIN=\u0026#39;https://\u0026lt;providedby\u0026gt;.orb.local\u0026#39; NEXT_PUBLIC_DEPLOYMENT_URL=\u0026#39;https://\u0026lt;providedby\u0026gt;.orb.local\u0026#39; Insight: These ORIGIN variables are the key to making the Convex dashboard and application routing work correctly behind a self-hosted proxy/gateway.\n3. Accessing the Dashboard Once all containers are up and running with the correct networking and environment variables, you need the administrative key to access the self-hosted dashboard.\nGenerate DB Access Key: This utility script is inside the database container:\ndocker exec \u0026lt;your-database-container-name\u0026gt; ./generate_admin_key.sh Convex Dashboard UI Login:\nURL: https://\u0026lt;providedby\u0026gt;.orb.local API Key: The key generated with ./generate_admin_key.sh. This process successfully brought up the full Convex stack, allowing for local development against a fully self-hosted database instance.\nüöÄ Conclusion: Own Your Stack Successfully navigating these networking hurdles demonstrates the power and flexibility of self-hosting. Taking the time to debug the communication between containers and properly define network origins turns a complex setup into a robust, local development environment.\nWhat\u0026rsquo;s your experience? Have you self-hosted similar backends in your home lab? Share your experience and any crucial configuration tweaks you discovered!\n","permalink":"https://liviuiancu.com/posts/convex-db-self-host/","summary":"Self-hosting modern application backends is often a puzzle of configuration, especially when working with newer architectures and tools. Convex, with its focus on full-stack development, offers a self-hosted option that\u0026rsquo;s perfect for a \u003cstrong\u003ehome lab\u003c/strong\u003e environment.","title":"üíª Convex Hosting on a Home Lab with Docker and Orbstack"},{"content":"Using the same prompt on four ai app generators to see how they compare. Main goal is to generate a voting web app.\nTested same prompt on 4 app generators Build a voting app that sorts a list on the votes received. The user is only allowed to vote after being logged in.\nDo not code just chat with me\nDo you have any clarifying questions that would help you deploy this request without bugs?\nAnswer me in great detail. Why do you think this will work\nLet me know if you understand what the task is before making edits\nTell me what you are going to do step-by-step and wait for my approval\nThe prompt is mostly inspired by this Reddit:\nhttps://www.reddit.com/r/ChatGPTPromptGenius/comments/1il4ias/i_built_over_20_apps_using_ai_tools_these_are_my/\nResults Strangely most clarification questions asked by AI looked remarkably similar across the services. I did not really check who used what AI provider under the hood. This are the pro and cons i observed. The best results i had were from Lovable, but it depends highly on the requirements. You have to try yourself.\nLovable Tech observed in source files: typescript, vite, tailwind, supabase\n+beautiful design, centered minimalist ui\n+sorting works nice based on votes and newest\n+notifications popup showing forbidden action\n+log in/out works\n+nice overview of db with users and items, appeared hashed\n+nice security scan that suggests fixes and docummentation-sign in required no mail confirm, fake\n-critical file .env containing DB details was not added to git ignore\nConclusion\nHappy days, not exactly what i wanted but good experience, good design over all, everything works as expected.\nV0 ( vercel ) Tech observed in source files: react js, tailwind, supabase\n+good clarifying questions, responded with about 6 detailed clarification points\n+sign in with google oauth sent confirm mail, appeared to work (only on first generated app)\n+overall smooth experience and good design\n-routing not working after login, had to guess route\n-did not find option to choose FE tech for generated results\n-nothing updates without refresh\n-voting counter jumps between 1 and -1\n-could not find where to see the contents of db\n-logs were not showing\n-generate is cumbersome, requires multiple click steps with wait between\n-after trying to generate same app a second time it went a lot worse:\n-generation stopped, i had to prompt wake it up, ended in some unhandled promise err, prompted again, login btn not reacting\nConclusion\nGood design, many bugs. Login appeared to work the first time, but after many ui bugs i tried generating the app a second time and now the login button was not reacting at all.\nPointed the problem to the bot 2 times and each time the ai confidently replied it is \u0026ldquo;fixed\u0026rdquo; but the button was still not reacting.\nReplit Tech observed in source files: vite, tailwind, radix-ui, postgresql?\n+creates check points without asking, nice\n+very nice design and sample datao\n-no route change when logged in / out\n-even tough i specified a login screen as landing page it displays content that should have been behind login\n-if you want to test a basic real auth, seems only option is to paywall for implementing \u0026ldquo;functionality\u0026rdquo;\n-not sure if any db was added since i only tested the free access\nConclusion\nGood basic design but the UX result was far from what I specified in clarification feedback (questions asked by ai). Navigation was non existant and content was displayed without the user being logged in.\nFlatlogic Tech mentioned: React, Node, tailwind, o3-mini model?\n+docker files generated\n+swagger api +nice additional recommended features to my initial prompt based on followup questions\n-lots of questions, got tired before generating anything\n-takes long time to show anything\n-slow ui, cant really inspect the whole code in free mode access\n-generated a blank screen \u0026hellip; no, just really slow start\n-cant sign in a new user\n-whole site seems buggy, i deleted my account and the website started refreshing repeatedly\nConclusion\nThe whole site was really slow, not just the generated app. Inspecting the whole code is not possible in the free tier. The ai asked so many clarification questions that I got tired and bored. In the end design was acceptable, but totally non functional buttons.\nSome conclusions, observations and especially warnings Danger: If you publish the code on github, in a public repo, there is high risk of exposing sensitive access tokens to your db or other integrated services. In some of the services the .env file was not added to .gitignore\nDebugging \u0026hellip; The machine does solve some small bugs fast, but when it fails you either enter an infinite prompting loop or you start learning the craft. Would rather download the source after its generated to an acceptable layout and continue in local env.\nProductivity \u0026hellip; i am a skeptic. It might be fun first, but after a while you notice the many, subtle ways, it fails with confidence, and needs careful review every time. For example: Ai outright forgets or ignores instruction like \u0026ldquo;add 6 sample items in database\u0026rdquo;, even when it asked if you want samples and you confirmed. After a while it asks again.\nAll negatives aside, it is spectacular what these ai generators were able to achieve with just one ambiguous prompt and 5 to 7 clarification followup questions. It allows for a lot of creativity, exploring ideas in multiple languages with tech stacks you never worked before.\nGreat tool if you know what you are doing, horrible giant foot gun if you dont\u0026rsquo; know anything about programming, secure communication between systems and trust the generated code with your wallet.\n","permalink":"https://liviuiancu.com/posts/comparing-ai-generators/","summary":"Using the same prompt on four ai app generators to see how they compare. Main goal is to generate a voting web app","title":"Comparing four ai generators generating a voting app"},{"content":"Reasons why I stopped using Azure and decided to use my home lab to build a portfolio and gain practical knowledge. Well one reason mostly, is cost. Big risk of incurring 500 to 1000+ euros cost per month.\nFor reference, using the Azure pricing calculator, Just one Kubernetes cluster with 4 virtual machines, running 15 days (lets say i don\u0026rsquo;t have time every day to tinker with it) can cost 200+ euros! Not counting additional resources like subnet, load balancer, gateway, storage, possible ingress, egress data, monitoring, and other gadgets I\u0026rsquo;m forgetting right now.\nFor that sum, I can easily buy cheap hardware with 32Gb RAM, decent CPU, SSD storage, and run more than one cluster, non stop with any additional services I would want to experiment with.\nGoal My goal is to learn how things connect to form a production infrastructure in the real world. CI-CD pipelines, load balancing, security, monitoring health checks, database backups, and have a good practical understanding of the cloud environment around a containerized web app. Basically DevOps and Kubernetes.\nI went through the AZ-400 Admin course. Found it difficult to follow, because of the very limited practical sandbox training. A lot of theory favoring power-shell commands but I\u0026rsquo;m used to Linux. The sandboxes started very slow or not at all. On top of that after 2 months in, somehow i got banned from opening the sandbox exercises and noticed a lot of people were receiving the same error.\nJust reading and clicking through the lessons slides is too much theory and won\u0026rsquo;t persist in memory. I attempted a az-400 simulated exam after that and no surprise, did not went well.\nCloud test run For more practice I opened an Azure account with free tier for one month, created some virtual machines, did a bit of networking, experimented with local Azure CLI and a few small Bicep templates. It was fun and that free month went by faster than you can type ip addr to view your network interfaces.\nI was very careful to delete all resources to not go over the 200$ limit for the free tier.\nIn life you easily get distracted. Its easy to forget to switch off and delete the resources every time you have a spare one hour in the evening after work to learn.\nAfter free tier expired, I was too stressed about accidental costs, considering the plan is to build a complete prod setup with ci-cd pipeline, monitoring, health check, load balancing and probably, Kubernetes later on.\nMy home lab So as an alternative to cloud for now, the best thing is to use my existing home lab to see how far I can push it without needing additional hardware. I have OPNsense as a next gen router and a Mac mini M4 16gb RAM that should be enough to explore the simplest setup of clusters.\nA home lab can be as complex as I can build it, no need to open it to the internet. I sleep well knowing i don\u0026rsquo;t have to remember to clean all resources, wait for them to be deleted and check again if i missed something still running outside of a resource group.\nCreating virtual machines in UTM seems to work very well. Tested Vagrant with UTM plugin. After 3 commands I had a Ubuntu VM with working ssh connection, generated from a template file.\nOf course Kubernetes at home is overkill when a simple container app will run just fine with software like Podman, Orbstack or Docker. My reasons for doing this are:\nProof of experience for career development\nTrying out Kubernetes self healing and automated rollbacks on Home Assistant and other services running at home\nThe power of creating multiple VM\u0026rsquo;s and containers with one command\nNext steps Plenty to do next, but most important step is to generate 4 virtual machines that will provide base for a Kubernetes cluster, test how the Mini M4 16gb can handle multiple running virtual machines. So far it proved great for running Home Assistant in a VM.\nThen will attempt to automate installing Kubeadm control-plane and nodes, try to run home assistant container version and follow how it behaves after connecting to a few devices.\nThat\u0026rsquo;s it for now, hope this will be part of a long series of Kubernetes journey.\nGet in touch if you know devops communities. Would also love to connect and share experiences with other people running a home lab with Kubernetes.\n","permalink":"https://liviuiancu.com/posts/homelab-alternative-to-cloud/","summary":"Reasons why I stopped using Azure and decided to use my home lab to build a portfolio and gain practical knowledge.","title":"Home lab vs Cloud provider: My quest for affordable, practical DevOps Kubernetes experience"},{"content":"Never imagined I would be writing about this. I built my own firewall / router using a small form factor PC with OPNsense open source firewall router. The previous \u0026ldquo;normal\u0026rdquo; router is now only used as wireless access to OPNsense.\nWhy did I do it? Its incredibly useful, practical and fun to learn something I am very interested in: blue team, SOC analyst perspective and improving network security for my home.\nI would probably go a lot further than a router, but in summer time, in my living space, having many devices to experiment with networking can quickly turn into a dry sauna and a suspicious electrical bill.\nWhy would you do this? Below are some practical benefits running OPNsense on your home network. It vastly improves safety for anyone at home using the internet.\nYou have powerful control over all devices,\nControl who has access to the network, when and for how long,\nLimit insecure IOT devices access\nFilter traffic in and out of the network,\nBlock adds for all devices,\nBlock known malicious websites,\nAutomatically scan the network for potentially malicious behavior,\nReceive prioritized alerts via email,\nAnd more\u0026hellip;\nA normal consumer router is no longer updated (patched for security) after 2 to 5 years. With OPNsense you have access to:\nRegular security updates for CVE's (common vulnerabilities and exposures), Features available in expensive commercial firewalls, Intrusion detection and prevention software like Suricata, Snort, Crowdsec, Zenarmour and many more. Read more about it at opnsense.org/about/about-opnsense\nThe amount of features you can run is only limited by your choice of hardware.\nWhat about the downsides? This is no walk in the park for someone with zero knowledge on IP addressing, subnets, firewall rules, DNS, network bridge and other networking notions. However I am also far from expert and learned a lot about networks along the way.\nThis setup is not a install and forget type of deal. You have to regularly:\ncheck for updates, check if any warnings popped up, sometimes a legitimate link might not work due to a false positive, ad-block lists can block legitimate links (I succeeded to block the mac os updates this way) A bit about the hardware. Initially I wanted a mini PC for small footprint, low power and low noise (this thing sits near my desk). Unfortunately it is very difficult to find parts for trusted mini PC brands, especially for what I needed: additional 4 port RJ45 network connections.\nSome mods I found were also very difficult to install.\nFun fact: some popular mini PC brands from china were shipped with malware preloaded from factory. Found this randomly when researching mini PC network upgrades.\nHardware requirements Mostly based on researching how to comfortably run intrusion detection and prevention software:\nMulti core CPU, low powered Have lots of storage for logs, min 250 Gb Minimum 8 gig RAM Have at least 4 nice RJ45 ethernet ports Run cool and quiet For the small budget assigned to this project, new hardware was out of the question, but found a good fit on the open market:\nA Dell Optiplex 5050 with 16Gb RAM, some 250GB ssd. Already had installed a 4 port network card, a nice i5-7400T CPU. Can easily be extended with RAM, more network cards or whatever else is compatible with the motherboard slots. The CPU type T is also a low power version perfectly suitable for running a PC 24-7.\nIt\u0026rsquo;s a bit overkill for my home lab, but the advantage of this is that it stays cool and quiet.\nHave you tried setting up your own firewall/router? Share your experiences and tips in the comments below.\nUntil next time, safe networking.\n","permalink":"https://liviuiancu.com/posts/router-opnsense/","summary":"Some practical benefits running OPNsense on your home network. It vastly improves safety for anyone at home using the internet.","title":"Building My Own Firewall Router with OPNsense: A Journey in Home Network Security"},{"content":"This article is the result of a random conversation, stirring my curiosity about how to colorize the Linux command line output. Yes I\u0026rsquo;m a geek.\nIdea: given the output of df -h command (shows disk usage in a neat table), color lines where disk usage is above some value you need start worry or take action.\nLast time I encountered the need to colorize terminal a few years ago, after installing zsh, I wanted to add more info to the terminal prompt.\nWas it required? No. Did I waste time? Yes. Was it fun? Definitely! So here I am again looking how to colorize command outputs this time.\nIf you think customizing things in the terminal cant be fun, checkout \u0026ldquo;there is a cow in my terminal\u0026rdquo; article: https://itsfoss.com/cowsay\nInitially i started out looking for any way to colorize the output of any command in a regular Linux terminal.\nAt the base of it there is the ANSI character escape sequence. It looks like this:\n\\033[XXXm Where XXX is a series of semicolon-separated parameters, not a movie rating. Checkout source for this and a great explanation here:\nhttps://stackoverflow.com/questions/4842424/list-of-ansi-color-escape-sequences\nThis ANSI thing is wild and an entire historical computer rabbit hole to get into: There is ANSI art\nhttps://en.wikipedia.org/wiki/ANSI_art\nBulletin board service in the 90'\nhttps://en.wikipedia.org/wiki/Bulletin_board_system\nText editors to create ANSI animations\nhttps://en.wikipedia.org/wiki/TheDraw\nAnd probably more but I\u0026rsquo;m getting hungry and want to finish writing this article so, moving on.\nAfter trying some simple things like below:\necho -e \u0026#39;hello! \\033[31m Colored text\u0026#39; which prints everything after \\033[31m in red text, I looked at comparing sed and awk. Both extremely powerful data manipulation commands. See one of many comparisons here:\nhttps://techcolleague.com/sed-vs-awk/\nBased on these comparisons I leaned more to awk for our current experiment of colorizing text based on multiple rules.\nFun fact! Or scary, depending on what you need to get done:\nawk is an entire programming language inside a command. There is a whole book about it:\nhttps://www.gnu.org/software/gawk/manual/gawk.html\nTrying AI tools. First i looked at the manual for awk and in the book mentioned, but why not throw in some recommendations from the robot.\nSince I never used awk, one of the mighty AI tools came in handy to generate a starter script and this is the result, but with a flaw. Can you see it?\ndf -h | awk \u0026#39;{ if ( $5 \u0026gt;= 80 ) { printf \u0026#34;\\033[1;33m%s\\033[0m\\n\u0026#34;, $0; } else { print; } }\u0026#39; The result of the coloring is wrong. It colors in yellow any row where the percentage of Use that is above 8, not 80.\nWhat a wonderful day to debug. And based on that statement, most of my days are wonderful. A bit about what I understand awk is doing:\nNumbers with letters or symbols attached are considered by awk to be of type string. This is why the script above fails.\nThe output data of df -h command looks like this sample. The \u0026ldquo;$5\u0026rdquo; is the fifth column, aka \u0026ldquo;5th field\u0026rdquo; in a \u0026ldquo;record\u0026rdquo; (text row).\nFilesystem Size Used Avail Use% Mounted on /dev/sda1 100G 10G 90G 10% / /dev/sda2 100G 60G 40G 60% / When asked about the flaw AI generated the following, which again, is not working:\nsubstr($5, 1, length($5)-1); This however, works. There is a + at the beginning for converting the string to a number\n+substr($5, 1, length($5)-1); Success, the conversion to number finally worked. This is the result:\nPartial capture of terminal output for the df -h command\nAdded some more teaks to keep the column formatting and spacing, and some default values.\nThis is the script if you want to try it out https://github.com/LiviuLvu/gist-some-scripting/blob/main/colorize_out.sh\nDon\u0026rsquo;t forget to chmod +x the file if you want to try out the script locally.\nSo that about solves my curiosity to color the lines of an output based on custom rules.\nWhat about you? Please share a link to fun customization you have seen or done for your terminal output.\n","permalink":"https://liviuiancu.com/posts/cmd-line-colors/","summary":"Coloring the output of a command","title":"Painting the command line: Coloring the terminal output to make more sense of data"},{"content":"Firefox plugin - Hide YouTube sections Hides given tags from DOM HTML view.\nhttps://addons.mozilla.org/en-US/firefox/addon/hide-recommendations-youtube/\nGoal: Small experiment to observe the effect of hiding recommendation sections and stop doom scrolling.\nHoping to see the following effects:\nSave Time Prevent loss of focus from original task Search for new content based more on deep thougt and internal motivation. Key tools: MutationObserver\nJavascript\n","permalink":"https://liviuiancu.com/posts/firefox-ext-yt-hide/","summary":"Small experiment to observe the effect of hiding recommendation sections and stop doom scrolling.","title":"Firefox plugin - Hide YouTube recommendation sections"},{"content":"A few weeks ago I got myself a little desktop that can serve as a home server‚Ä¶ some 16Gb DDRam3, 2 SSD 60Gb each, 2 Tb HDD + 1Tb HDD‚Ä¶ hmm, lots of serving possibilities :)\nMission? Unclear .. wait, a file server would be nice to have.\nActually the main reason for this headache is to learn about servers and networks, because in frontend (my job) there were many situations where I was confused by errors involving network requests, errors from server, network and so on.\nUsed the word headache because that is what it turned into, but man, I learned a lot about computer networks and working in the command line linux. Which reminds me of‚Ä¶\nA great book on networks Oh, and I started reading from High Performance Browser Networking from Ilya Grigorik. Fantastic resource, great help to understand how networks work. There are so many things that can go wrong on the data transfer path. https://hpbn.co/#toc\nWhile searching for tools and protocols to share files I found OwnCloud and proceeded to install all the required packages including the kitchen sink and then uninstall, clean and reinstall some packages a couple of times :) Errors were attacking from every direction.\nAs a new comer to Linux and servers, saying it was hard is an understatement. After gatering a lot of documentation I figured it would be worth while to write an article about it.\nThese are some of the commands i encountered:\nCentos7 useful commands nmcli \u0026gt; command line tool for controlling network manager\nnmtui \u0026gt; text ui for controlling network manager\nnmap \u0026gt; Network exploration tool and security\nlsof \u0026gt; list open files\nnetstat \u0026gt; Print network connections, routing tables, interface statistics, masquerade connections, and multicast memberships ifconfig \u0026gt; configure a network interface\nip \u0026gt; show / manipulate routing, devices, policy routing and tunnels\nlshw \u0026gt; list hardware\nip addr show \u0026gt; check the current IP address\nip r \u0026gt; show default gateway\nhostnamectl \u0026gt; display host name + info\nyum install links \u0026gt; ‚ÄúLinks‚Äù command line web browser (this was fun)\nsudo top \u0026gt; list proceses running on server\nip addr show eno1 | grep inet | awk ‚Äò{ print $2; }‚Äô | sed ‚Äòs//.*$//‚Äô \u0026gt; shows public ip address\nnslookup yourhost.yourdomain.com \u0026gt; Is the DNS server returning the right IP ?\nnetstat -anp | grep 443 \u0026gt; what is using port 443\nfuser 443/tcp \u0026gt; what is using port 443\nnetstat -lnp | grep 443 \u0026gt; what is using port 443\nkill -9 #### \u0026gt; stop process number: #### using port 443\nss \u0026gt; is a useful power tool to inspect various statistics for open sockets\nss ‚Äî options ‚Äî extended ‚Äî memory ‚Äî processes ‚Äî info \u0026gt; to see the current peers and their respective connection settings\nPackage Groups options instructions https://www.certdepot.net/rhel7-get-started-package-groups/ About PHP server test and details nano /var/www/html/info.php https://localhost/info.php \u0026gt; File should be deleted after test\nrpm -qa | grep -i [RPM_PACKAGE_name] \u0026gt; search for rpm package rpm -e [RPM_PACKAGE_name] \u0026gt; delete it\nServer setup My server setup currently looks like this: CentOS 7 Own Cloud Apache server MariaDB\nFinally got this contraption to start. SoundCloud is merrily serving files but only inside LAN. After doing some port forwarding and static IP asignment shenanigans inside the router and following Centos settings tutorials, Apache is able to serve files outside LAN through DDNS servers from noip.com\nThere is much more to be learned, especially security stuff, but I will stop for now. This rabbit hole is getting deep and I also have a lot of frontend subjects to experiment.\nHave fun and keep exploring!\n","permalink":"https://liviuiancu.com/posts/centos7-owncloud/","summary":"Lessons from High Performance Browser Networking from Ilya Grigorik. Fantastic resource, great help to understand how networks work.","title":"Setting up a home CentOS 7 server with OwnCloud"},{"content":"There is this book I have on my desk called ‚ÄúBrainmatics Logic Puzzles‚Äù from Ivan Moscovich. A few days ago I opened it on the page about Kaprekar‚Äôs Digitadition pattern. That looked very interesting and made a few lines in Illustrator to see the pattern.\nAfter drawing a few lines I decided to generate the pattern as large as possible and see what happens with large numbers.\nThis digitadition pattern made by selecting a number and adding to it the sum of its digits.\nExample: take number 25, add 2+5, you get 28, and repeat 28, 38, 49, 62, 70, 77, 91, 101, 103, 107, 115\nView live demo below Live demo on github: Link to live demo\nhttps://liviulvu.github.io/d3-kaprekar-pattern/\nGet the code made for this demo here: Code Files\nhttps://liviulvu.github.io/d3-kaprekar-pattern/\nThe interface is zoomable and scrollable thanks to D3 but I did not take the time to fix some UI quirks. For example you can scroll off the screen and there is no reset button to center back the shapes.\nThe pattern looks a bit like data embedded on a tape, so I searched the web to find ways to convert this pattern to sound. Found two resources for this: One is Photosounder (Windows), which is a software that can generate sound from images.\nhttp://photosounder.com/\nThe second is a website that can generate sound from number patterns, called MusicAlgorithms.\nhttp://www.musicalgorithms.org/3.2/\nOk, this was fun, but cant think of anything useful coming out of this experiment. Moving on.\n","permalink":"https://liviuiancu.com/posts/math-d3-visualisation/","summary":"Visualising patterns using the D3 library.","title":"Mathematical Pattern Visualization with D3"},{"content":"This is a big book on performance for web developers. I am writing this article especially for myself as a reminder of the many things that can be improved in our apps and servers.\nWhat this book is about? In the words of the author:\n‚Äú How the network works, why it works the way it does and what you can do to optimize it ‚Äù Key points and lessons learned Understand bandwidth (measured in Mbps) vs latency (measured in ms). Need to transfer many small pieces? Latency is priority. Need to send big files over same pipe others are transferring big files? Bandwidth is more important.\nThroughput indicates the level of successful packet delivery from one point on the network to another.\nLatency is the performance bottleneck for most websites\nFor the user, once a 300 millisecond delay threshold is exceeded, the interaction is often reported as ‚Äúsluggish‚Äù.\nMost modern browsers, both desktop and mobile, can open up to six connections per host.\nThe fastest request is the one not made. Reduce transfer size, reuse cached content on client, reuse connections.\nThe rate with which a TCP connection can transfer data is often limited by the round-trip time between the receiver and sender.\nTCP is optimized for long-lived connections and bulk data transfers.\nUDP or Unreliable datagram protocol is a faster, simplified version of the TCP protocol. It does not provide any guarantee of message delivery, order delivery, state tracking and congestion control. UDP needs a lot of work to be optimized so it is more convenient to use WebRTC (real time communication).\nSSL (secure sockets layer) developed at Netscape to enable e-commerce transaction security on the web standardized as TLS.\nTLS (transport layer security) protocol designed to provide three essential services to all applications running above it: encryption, authentication, and data integrity\nThe best way to minimize both latency and computational overhead of setting up new connections is to optimize connection reuse.\nTo get the best security and performance guarantees it is critical that the site actually uses HTTPS to fetch all of its resources.\nIntermediaries between your source and destination ‚Äúnew protocols and extensions to HTTP, such as WebSocket, HTTP/2, and others, have to rely on establishing an HTTPS tunnel to bypass the intermediate proxies and provide a reliable deployment model: the encrypted tunnel obfuscates the data from all intermediaries.‚Äù\nOptimizing for mobile phones Phone‚Äôs radio component is often second in power consumption only to the screen.\nMinimize use of radio interface. Do not fire keepalive signals if not absolutely necessary, it can drain the battery very fast.\nDo not couple user interactions, user feedback, and network communication. If a network request is required, then initiate it in the background, and provide immediate UI feedback to acknowledge user input.\nSignal availability is very unpredictable and your app should be able to work offline.\nGroup your requests together and transfer as much as possible, then let the radio return to idle\nHTTP 1.1 features keep-alive connections, chunked encoding transfers, byte-range requests, additional caching mechanisms, transfer encoding, and request pipe-lining.\nHTTP 2 features many enhancements such as compression of HTTP header fields, request prioritization and server push, stream prioritization, request and response multiplexing.\nSince HTTP 2 works with streams it needs different optimizations techniques compared to HTTP 1. Chapter Optimizing Application Delivery offers a great list of optimizations for each.\nXHR used for async request. Not suitable for real time apps.\nSSE only server-to-client streaming of text-based UTF-8 real-time data. Binary data transfer is inefficient\nWebSocket allows bidirectional communication over the same TCP connection. Requires careful performance tuning. Real-time push can bring a big cost to mobile devices.\nWebRTC for Peer-to-peer audio and video streaming. Direct one to one communication can be simple, but it gets much more complicated between multiple peers.\nThe book is available online here.\nhttps://hpbn.co/\nOther tools and resources Measure performance with the Navigation Timing API. Offers metrics that are not accessible with dev tools.\nhttps://developer.mozilla.org/en-US/docs/Web/API/Navigation_timing_API\nHigh Performance Networking in Google Chrome https://hpbn.co/chrome-networking\nIn conclusion There are many more useful lessons if you read the book. I found the WebRTC chapter to be the most difficult one to read because of all the details involving the optimizations of the UDP layer.\nIf you did not read this book yet, it is really worth the effort of going through the 400 pages. It was really interesting to read about how the mobile networks work, what the OSI model looks like for the web and what happens to your precious packets or datagrams in their journey across different environments.\nHave fun learning!\n","permalink":"https://liviuiancu.com/posts/book-on-browser-network/","summary":"\u003cp\u003eThis is a big book on performance for web developers. I am writing this article especially for myself as a reminder of the many things that can be improved in our apps and servers.\u003cbr\u003e\n\u003cimg alt=\"Osi model\" loading=\"lazy\" src=\"../../assets/img/image.png\"\u003e\u003c/p\u003e\n\u003cp\u003eWhat this book is about? In the words of the author:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e‚Äú How the network works, why it works the way it does and what you can do to optimize it ‚Äù\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id=\"key-points-and-lessons-learned\"\u003eKey points and lessons learned\u003c/h3\u003e\n\u003cp\u003eUnderstand \u003cstrong\u003ebandwidth\u003c/strong\u003e (measured in Mbps) vs latency (measured in ms). Need to transfer many small pieces? Latency is priority. Need to send big files over same pipe others are transferring big files? Bandwidth is more important.\u003c/p\u003e","title":"Notes on the book High Performance Browser Networking"},{"content":"Have you ever tried to explain something that could be very helpful but seems too complicated to others because you did not properly present your idea?\nIn this article I want to tell a story about my mistake in proposing a (kind of complicated) solution to optimize the build process for the front end team I am part of. I will have to leave out a lot of juicy details about the project because it‚Äôs still ongoing work.\nThis is the plot We have a pretty nasty build that can take more than 2 minutes to complete. If not restarted often, after each change, this time bending monster can grow to 4 minutes or more. Then send the css through CORS to a remote machine. There are other limitations involving security and limited access to part of the code base.\nImagine yourself try to style a few tabs or for any UI work your heart desires and then waiting like a monk to see those changes in the browser.\nIn this build recipe we throw 4 theme color files (among other tech). One of them is the old version that we are working hard to replace in the hopefully near future (its a big whale of a project). At this time we only need 2 of these themes.\nThe themes are developed using sass files and besides their processing there are also some internationalization files and other javascript files that we don‚Äôt touch in our css beautifying process.\nThe idea Many moons ago (few months back) we had 2 themes and the build was ‚Äúonly‚Äù 30 seconds. Now we have 4. So until we finalize the layout and color variables why not keep the 2 new themes out of the build?\nOh, how wonderful life could be‚Ä¶ Well, kind of: 30 seconds is not that great but is much better than 2+ minutes to wait for a darn padding change.\nWe have to do something In order to achieve this outstanding tech innovation (temporary remove files from the build) we first discussed it in a meeting with ‚Äúhigher order council‚Äù. I think we got a ‚Äúwe will look into it‚Äù response but removing the 2 extra themes was off the menu.\nMeanwhile we keep adding css and the build time keeps growing. None of us on the front end team really understands all the build settings involving .sh script files that connect our color themes and other contraptions that our project is connected to (‚Ä¶long story). We get by with testing css changes in dev-tools and copying them in the project files.\nBeing a curious fella, I started looking into how the theme files are connected with the .sh script files. After poking around (at this time bash scripting is too gibberish for me) i hacked a way to remove 2 themes and shorten build and watcher update times.\nJolly good, good times ahead, but wait, I have to share this change with front end team and we are not allowed to push any of these changes to main branch.\nTo get around the updating branch restriction, I did some git ignore and git update-index ‚Äî assume-unchanged commands to make git ignore changes to the removed theme files. In my opinion not that many or difficult changes but I was so wrong.\nThis is where I was reminded that people need simple solutions to understand the concept and take action.\nI‚Äôm not a very good sales man Actually I probably suck at selling because the sharing plan did not go well and the optimization was received as complicated.\nShorter build time - useful stuff, right? I had a go at letting everybody know I am working on shenanigans that would make our build faster and everyone was interested.\nSo there I go, building this wonderful instructions doc (the kind of user manual nobody reads), with screen shots and git commands (a painting of rotten apples might have been better received).\nThe magic vanished when the guys and gals visited the instructions doc. I was expecting resistance so I went and simplified a lot of the file and removed as much text and images as possible while keeping the essential. After announcing the upgrade, based on feedback no one asked any questions and it was clear I failed to present a usable, practical solution.\nOne more try Only one hope remains to make the build optimization steps look attractive (more like push this button instead of read this user manual). If I can use bash scripting to automate all those changes in one command then it just might work for everyone.\nIf you have a similar story please share.\nTo be continued.\n","permalink":"https://liviuiancu.com/posts/build-optimization/","summary":"Have you ever tried to explain something that could be very helpful but seems too complicated to others because you did not properly present your idea?","title":"Failing to sell a build optimization idea and not giving up"},{"content":"Starting a local node without any java experience\nThe documentation looks pretty good at first sight but i would not call it beginner friendly.\nStarted with this page https://docs.ergoplatform.com/node/install/ to install a ergo node but it quickly turned into 2 days of searching for alternatives and tutorials.\nNothing worked as easy as described in the docs The long command on the install page to start the node needs to be adjusted, wait, it was adjusted while I was writing this article :) The file name should be exactly as the jar file you download but it was not indicated in the command with \u0026lt;\u0026gt; or described in instructions.\nNo downloaded file worked, tried 5 times with different jar files. Each time got the corrupt jar file ‚Äúerror: Invalid or corrupt jarfile ergo-‚Ä¶..jar‚Äù\nThe script to start a node was infinitely loading with message: ‚ÄúExecuting ergo node to obtain API key hash ‚Ä¶‚Ä¶‚Ä¶.‚Äù\nI tried to change the java version (on some discussions it was a case that solved the corrupted file error)\nChanging the java version was quite trippy, as in tripping over every step. Installing v11 on ubuntu is really easy, any version is really easy to install but changing the version is another story. For 2 hours i was fighting with all my will power to change active version 11 to newly installed version 17. No command or restart of terminal did any difference on java \u0026ndash;version result. Eventually manually deleting the v11 folder finally made the difference ‚Ä¶ moving on.\nSome things in the documentation remain to be interpreted based on experience with Java and Scala which in my case was close to zero.\nAfter 2 days i gave up and it was another week until i had time and patience to try again. Its a bit confusing but there are 2 pages with similar, but not the same set of instructions for starting a node so i sort of made a trial and error with both: 1 https://docs.ergoplatform.com/node/install/faq/#running-the-node\n2 https://github.com/ergoplatform/ergo/wiki/Set-up-a-full-node#running-the-node-for-the-first-time The sections prerequisites and running the node are of interest for this experiment.\nThese are the steps that worked to get a node going on Ubuntu OS Download the ergo repository from Github: git clone https://github.com/ergoplatform/ergo.git Change directory inside that folder:\ncd your/downloaded/ergo/folder Install the scala build tool: https://www.scala-sbt.org/download.html and compile the node Check installation running command:\nsbt --version Result output should be similar to:\nsbt version in this project: 1.6.2 sbt script version: 1.6.2 Compile the ergo jar file with command:\nsbt assembly Run the ergo node. Create a new folder. For my example named ergo-node (creative right‚Ä¶) Navigate to the new folder:\ncd ~/ergo-node Create new file ergo.conf as instructed in the running node section in the docs: https://docs.ergoplatform.com/node/install/faq/#running-the-node\nStart the node using the generated jar file when you followed step 2:\njava -jar -Xmx4G /your-downloade-ergo-folder/ergo/target/scala-2.12/ergo-\u0026lt;your-version\u0026gt;.jar ‚Äî mainnet -c ergo.conf After compiling the jar file the command for starting the node finally worked and did not throw any corrupted .jar file error. Dont‚Äô remember why I created a separate folder where to run the node but this proved useful for running the node. The logs are saved there and some new folders and files are created.\nOff topic: accidentally stopped the node sync 4 times The interrupt command in the Ubuntu terminal by default is ctrl + c, exactly what is used for copy paste text in other windows. Accidentally stopped the node sync, by attempting to copy parts of the logs which kept displaying errors similar to: failed to connect to , connection refused or connection timeout.\nTo avoid accidentally stopping any running processes, the command to change the default copy shortcut ctrl + c interrupt command in Ubuntu terminal is this:\nstty intr ^i The sync of the node seems to take a lot of hours or days depending on network connection. Cant‚Äô wait for the full sync to start experimenting some more with the api.\nI hope this posts saves you some head pills and sleep hours. Let me know if its useful for you and if you have anything to add or spotted any errors, please leave a comment to guide new comers on the correct path.\nHave fun experimenting!\n","permalink":"https://liviuiancu.com/posts/ergo-blockchain/","summary":"Started with this page \u003ca href=\"https://docs.ergoplatform.com/node/install/\"\u003ehttps://docs.ergoplatform.com/node/install/\u003c/a\u003e to install a ergo node but it quickly turned into 2 days of searching for alternatives and tutorials.","title":"Trying out the Ergo blockchain environment"},{"content":"In the last couple of months I became curious about what makes a crypto currency work, and why I see more and more news about the technology behind it, promoting it as the future solution to so many problems. How does blockchain work?\nOne of my paths in blockchain land went to, what I thought would be an easy start (I was a tad wrong). Learning the basics by trying out the truffle tutorial ‚Äúpet-shop‚Äù.\nhttps://trufflesuite.com/tutorial\nIf you thought the frontend environment was changing fast and there are a lot of frameworks popping up every week, wait till you get your fingers dirty writing some blockchain code.\nNode version troubles With versions. Thank god that nvm exists. Tried node 8, 10, 12, 14, 16 then npm install, install issue, something about the contracts was just not working as expected‚Ä¶\nTried a bunch of solidity compiler versions until one of them compiled the damn contracts without the issue: ParserError: Source file requires different compiler version.\nWhen you see a command like npm install -g truffle you (or maybe you do) but I did not realize the tutorial is a bit old and that install command will use the latest truffle version. Some more headache for me, yaay!\nI switched versions until some more neurons died and the remaining ones realized the truffle and solidity compiler versions needed to match with the old tutorial.\nThe final combination that worked then was:\ntruffle@5.2.0 | solidity compiler v0.5.16 | node v16 (older versions of node worked in the end, after the compiler and truffle versions were compatible with the contracts)\nDon‚Äôt expect it to work now when you are reading this :)\nThis is the code base after installing everything mentioned in the tutorial:\nhttps://github.com/LiviuLvu/etherum-solidity-pet-shop\nAditionally, to the tutorial code, I added some node packages like Prettier and Husky to format the code, because I think it‚Äôs a chore to format the code manually, in a consistent way, so I like to have this done automatically on save and before git commit hook.\nPro tip from:\nhttps://trufflesuite.com/guides/debugger-variable-inspection/#debugging-errors\n‚ÄúSometimes, contracts have errors in them. They may compile just fine, but when you go to use them, problems arise.\nThis is one of the reasons to use Ganache over a public testnet: you can easily redeploy a contract without any penalty in time or ether.‚Äù Eventually All the parts were working together. From the local repo the deploy to Ganache local network worked.\nThe Ganache UI is very nice and after deploying a contract or adopting a pet, was showing a lot of new info that I did not understand at first.\nThe Metamask wallet was connected to Ganache and was showing the change in the amount of coin when adopting a pet.\nOverall Going through the Truffle tutorial was worth it as a quick way to get a general idea of the Ethereum environment. Totally recommend it, even with the difficulties with versions mentioned above.\nFor the following experiments I am looking at Remix, or try out the environment from Hardhat. There are a lot of good views about their tools.\nMaybe after trying the Hardhat tutorial it would be nice to write a comparison article with Truffle?\nUntil then, have fun learning!\n","permalink":"https://liviuiancu.com/posts/blockchain-learning/","summary":"In the last couple of months I became curious about what makes a crypto currency work, and why I see more and more news about the technology behind it, promoting it as the future solution to so many problems. How does blockchain work?","title":"Learning blockchain programming"},{"content":"This article is dedicated to the misfits, the outliers, the‚Ä¶ just kidding. It is for you if the importance of unit testing is unquestionable and you are looking for unit testing references with examples of code.\nBelow is a demo project that is using Jasmine for testing Angular. It is up and running, ready to tweak, experiment, break apart and let your imagination explore it.\nIt can be very useful to use as boilerplate in case you need to demo a concept related to your main project, or ask a question on stack overflow and provide live code to help test the solution.\nhttps://stackblitz.com/edit/angular-unit-testing-examples\nThe above stackblitz project is also available on github. After download you can run the tests locally.\ngit clone https://github.com/LiviuLvu/angular-unit-testing-examples.git npm run start npm run test Best practice notes Don‚Äôt test implementation details.\nRecommended size is 10 lines of code or less.\nWrite expected functionality of a component first.\nReuse code as much as possible.\nDescription must be easy to read.\nCode coverage recommended is minimum 80%.\nTests should run fast (especially on large projects).\nAvoid complicated logic.\nTest structure recommended Divide your test method into three sections:\n1 2 3 4 // Arrange let count = 0;// Act const newCount = increment(count);// Assert expect(newCount).toBe(1); Tests should not be fragile: Lets say we test a line that creates some string.\n1 let someString = `Active user is ${name}` You can test only the dynamic content. In case the rest of the string is changed, it will not break test functionality.\n1 expect(someString).toContain(\u0026#39;Some Name\u0026#39;); What‚Äôs in a good test failure bug report: What were you testing?\nWhat should it do?\nWhat was the output (actual behaviour)?\nWhat was the expected output (expected behaviour)?\nReferences and mentions Articles\nAn Overview of JavaScript Testing in 2019\nhttps://medium.com/welldone-software/an-overview-of-javascript-testing-in-2019-264e19514d0a\nWhat every unit testing needs\nhttps://medium.com/javascript-scene/what-every-unit-test-needs-f6cd34d9836d\nAngular unit testing 101 (with examples)\nhttps://dev.to/mustapha/angular-unit-testing-101-with-examples-6mc\nVideo\nKent C. Dodds ‚Äî Write tests. Not too many. Mostly integration https://youtu.be/Fha2bVoC8SE?t=105\nUnit Testing in JavaScript and Jasmine | TLDR Jasmine Unit Test Tutorial By: Dylan Israel\nhttps://www.youtube.com/watch?v=h2eWfvcAOTI\nTypes of tests\nFunctional Testing types include\nhttps://www.softwaretestinghelp.com/types-of-software-testing/\nUnit Testing, Integration Testing, System Testing, Sanity Testing, Smoke Testing, Interface Testing, Regression Testing, Beta/Acceptance Testing\nNon-functional Testing types include\nhttps://www.softwaretestinghelp.com/types-of-software-testing/\nPerformance Testing, Load Testing, Stress Testing, Volume Testing, Security Testing, Compatibility Testing, Install Testing, Recovery Testing, Reliability Testing, Usability Testing, Compliance Testing, Localization Testing\nIndex Fixture: A fixture is a wrapper around an instance of a component. With a fixture, we can have access to a component instance as well as its template.\nMock: Mock objects are fake (simulated) objects that mimic the behaviour of real objects in controlled ways.\nSpy: Spies are useful for verifying the behaviour of our components depending on outside inputs, without having to define those outside inputs. They‚Äôre most useful when testing components that have services as a dependency.\n","permalink":"https://liviuiancu.com/posts/unit-tests-angular/","summary":"For you if the importance of unit testing is unquestionable and you are looking for unit testing references with examples of code.","title":"Unit tests examples in Angular with Jasmine"},{"content":"In this article I write a bit about using the power of scripts to help my comrades automate repeated tasks and process optimizations.\nThis is a follow up to previous article, where i wrote about a fail that gave me the motivation to learn bash scripting. I tried to do some build optimizations manually and they worked but there were too many steps to be a shareable solution.\nMake it friendly When you share a script with a fellow programmer, put some friendly ASCII art in there so it will appear in the command line to signal the end of the execution. It makes things more lively and fun.\nI placed an elephant in a script file, shared with team and wrote them that all is well if they see an elephant at the end of the process.\nSharing a bunch of steps automated in a script file was my starting motivation.\nLearning to code in bash was useful and fun but very intimidating at first. After getting many syntax errors and sometimes worrying that I will break things in irreparable ways I got the basics and started automating whatever I could. Below are solutions shared at work.\nAutomation ideas Here are some ideas we tried and made our tasks flow more like the ki energy in a zen garden:\nBuild optimization to temporarily modify some files that we were not allowed to modify (not solvable with git ignore).\nMorning grind routine when we needed to re-connect our local dev environment to a virtual remote machine.\nUpdate a branch with latest changes while still working on it (a bunch of git commands mashed together in one script).\nAutomate daily ssh commands where the script autocompletes command line questions where needed (ex. login details).\nThe best part of making and sharing a script was that it sparked curiosity about the possibilities of automation and how we could use it in our daily workflow.\nFun Fact Did you know there are games you can play in the command line?\nMore to come: Node.js While searching for automation ideas for my workflow i discovered that using Node instead of bash scripting could be even more rewording as there are many packages on NPM to choose from and node is cross platform.\nFound great book on the subject of automating stuff using node: Automating with Node.js by Shaun Michael Stone. Book goes on to do list!\nThe reword for succeeding after a fail and helping others is treasure for the soul. I hope this article inspires you to explore, discover and improve your every day.\n","permalink":"https://liviuiancu.com/posts/automate-tasks/","summary":"A bit about using the power of scripts to help my comrades automate repeated tasks and process optimizations.","title":"Automate tasks and share using script files"},{"content":"One of the projects I was working on has the default pull-request option set to master. For whatever reason I kept forgetting to set this to develop branch. It just felt like it would stay set to develop once i created a PR with that setting‚Ä¶silly me.\nGetting frustrated with my forgetfulness i searched for a more convenient, alternative way to send pull requests from the command line. This is how I found\ngit pull-request https://github.com/jd/git-pull-request\n‚Äògit-pull-request is a command line tool to send GitHub pull-request from your terminal‚Äô\nNaturally like any experiment i try, it is too good to be true when it works just by copy-pasting the commands from the instructions, because i don‚Äôt understand a lot of stuff in those instructions.\nI got some missing .netrc file error. Bummer. Now I told myself: OK, if it takes me more than 10 minutes to get this pull-request from the command line working I will skip this experiment and move ahead with more important stuff on my ‚ÄòTo learn list‚Äô (darn list, it‚Äôs starting to look like a sky scraper).\n16 minutes later ‚Ä¶ multiple tabs open, git settings files, .gitconfig, .netrc, security issues, more confusion ‚Ä¶ time out.\nBeyond a point I get to involved in the activity, or rather too attached to the idea to give up. Especially if it would increase productivity and remove manual input. In this case, after a break I returned to the screen and found clarity.\nSetup shenanigans Below I will detail some of the steps to setup that worked for me on mac Tried to follow the instructions on: https://github.com/jd/git-pull-request\nInstallation ‚Äî this went like a charm: pip3 install git-pull-request\nBut it worked for me because I already had Python3 installed during a weekend Hackaton :) that also took some focus to set up correctly. This command line tool needs a config file .netrc placed in user path ~/ It looks like this:\nmachine github.com editor nano login \u0026lt;git user name\u0026gt; password \u0026lt;personal access token\u0026gt;machine api.github.com editor nano login \u0026lt;git-user-name\u0026gt; password \u0026lt;personal access token\u0026gt; I tried it without editor and it kept throwing some error:\n$EDITOR is unset, you will not be able to edit the pull-request message So i thrown in that setting to make it go away.\nThe personal access token can be your plain text password but it is not recommended. Instead you can generate a token in git \u0026gt; developer settings \u0026gt; Personal access tokens \u0026gt; Generate new token Some also recommended to encrypt the folder where you keep these settings\nAfter you have .netrc file in place you should be able to use the git pull-request command.\nI used it like this: *switch to local branch with commits pushed to remote branch, ready for PR\ngit checkout \u0026lt;local-branch-tracking-upstream-branch\u0026gt; git pull-request ‚Äî target-branch develop ‚Äî no-rebase Git pull-request options usage: git-pull-request [-h] [ ‚Äî download DOWNLOAD] [ ‚Äî debug] [ ‚Äî target-remote TARGET_REMOTE] [ ‚Äî target-branch TARGET_BRANCH] [ ‚Äî title TITLE] [ ‚Äî message MESSAGE] [ ‚Äî no-rebase] [ ‚Äî force-editor] [ ‚Äî no-comment-on-update] [ ‚Äî comment COMMENT] [ ‚Äî no-tag-previous-revision] Send GitHub pull-request. optional arguments: -h, ‚Äî help show this help message and exit ‚Äî download DOWNLOAD, -d DOWNLOAD Checkout a pull request ‚Äî debug Enabled debugging ‚Äî target-remote TARGET_REMOTE Remote to send a pull-request to. Default is auto- detected from .git/config. ‚Äî target-branch TARGET_BRANCH Branch to send a pull-request to. Default is auto- detected from .git/config. ‚Äî title TITLE Title of the pull request. ‚Äî message MESSAGE, -m MESSAGE Message of the pull request. ‚Äî no-rebase, -R Don‚Äôt rebase branch before pushing. ‚Äî force-editor Force editor to run to edit pull-request message. ‚Äî no-comment-on-update Do not post a comment stating the pull-request has been updated. ‚Äî comment COMMENT, -C COMMENT Comment to publish when updating the pull-request ‚Äî no-tag-previous-revision Preserve older revision when pushing Also tried\ngit pull-request ‚Äî target-branch master ‚Äî no-rebase ‚Äî force-editor But this throws error:\nbad follower token ‚Äòeditor‚Äô (/Users/my.user/.netrc, line 2) I wrote the people from https://github.com/jd/git-pull-request If you know how to fix this please send a message.\nThat‚Äôs all for this experiment. Hope this helps you to be more productive. Have fun!\nUpdate:\nTo fix the editor is unset error, add this line: export EDITOR=nano\ninside this file: ~/.bash_profile (mac OS)\nand now whenever I write the PR command, the nano editor opens and‚Ä¶ behold! I can edit the PR request message.\n","permalink":"https://liviuiancu.com/posts/git-pull-from-shell/","summary":"A more direct pr request init from terminal","title":"Git pull-request from command line"},{"content":"About After discovering the power of scripting automations in software as a graphic designer, I got really interested in programming.\n","permalink":"https://liviuiancu.com/about/","summary":"\u003ch2 id=\"about\"\u003eAbout\u003c/h2\u003e\n\u003cp\u003eAfter discovering the power of scripting automations in software as a graphic designer, I got really interested in programming.\u003c/p\u003e","title":""}]