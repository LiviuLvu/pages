<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Blog chat based on RAG AI with local small LLM | Dev Pages</title>
<meta name=keywords content="rag app,ai app,chat bot,documents chat"><meta name=description content="This is my first attempt to build a chat bot that answers only based on found documents. In this case my blog posts."><meta name=author content="Liviu Iancu"><link rel=canonical href=https://liviuiancu.com/posts/chat-blog-rag-app/><link crossorigin=anonymous href=/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as=style><link rel=icon href=https://liviuiancu.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://liviuiancu.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://liviuiancu.com/favicon-32x32.png><link rel=apple-touch-icon href=https://liviuiancu.com/apple-touch-icon.png><link rel=mask-icon href=https://liviuiancu.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://liviuiancu.com/posts/chat-blog-rag-app/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://liviuiancu.com/posts/chat-blog-rag-app/"><meta property="og:site_name" content="Dev Pages"><meta property="og:title" content="Blog chat based on RAG AI with local small LLM"><meta property="og:description" content="This is my first attempt to build a chat bot that answers only based on found documents. In this case my blog posts."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-02-26T00:00:00+00:00"><meta property="article:modified_time" content="2026-02-26T00:00:00+00:00"><meta property="article:tag" content="Rag App"><meta property="article:tag" content="Ai App"><meta property="article:tag" content="Chat Bot"><meta property="article:tag" content="Documents Chat"><meta property="og:see_also" content="https://liviuiancu.com/posts/summary/"><meta name=twitter:card content="summary"><meta name=twitter:title content="Blog chat based on RAG AI with local small LLM"><meta name=twitter:description content="This is my first attempt to build a chat bot that answers only based on found documents. In this case my blog posts."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://liviuiancu.com/posts/"},{"@type":"ListItem","position":2,"name":"Blog chat based on RAG AI with local small LLM","item":"https://liviuiancu.com/posts/chat-blog-rag-app/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Blog chat based on RAG AI with local small LLM","name":"Blog chat based on RAG AI with local small LLM","description":"This is my first attempt to build a chat bot that answers only based on found documents. In this case my blog posts.","keywords":["rag app","ai app","chat bot","documents chat"],"articleBody":"Providing a RAG (Retrieval-Augmented Generation) AI for my blog.\nThe goal: a privacy-first, locally hosted assistant that only answers based on my posts. No external APIs, no subscriptions ‚Äî just my hardware and local models.\nBelow are steps I went through in the building process, requiremets, whishlist roadmap, errors and proposed solutions. This article was built more like a reference than a story so feel free to jump where you need.\nTable of Contents The Tech Stack Architecture \u0026 Query Flow Implemented RAG Features Future Roadmap Core Components Document loader Text splitter Embeddings Vector Store Retriever Prompt template LLM model query UI for the RAG app Deployment to Proxmox Problems and solutions to RAG search Thoughts about the RAG chat The Tech Stack Component Technology UI Streamlit Orchestration LangChain Vector Store Chroma DB (Local disk persistence) Embeddings Local small embeddings model LLM Local model on Ollama Architecture \u0026 Query Flow The request travels from blog to the local LLM\nBlog -\u003e Cloudflare URL -\u003e cloudflared -\u003e RAG Container (UI) -\u003e LLM Container (Ollama)\nIntegration with blog as a simple link on first page or separate page containing link to a local running container.\nImplemented RAG Features ‚úì Only answer based on blog posts. Say i cant find relevant knowledge if no context found\n‚úì Should work local, no remote calls or subscription required\n‚úì Be as simple as possible in the beginning.\n‚úì Citations - point to source file\n‚úì Vector store: store vectors embeddings from text in a local memory database\n‚úì Build a simple user interface using Streamlit UI.\n‚úì Update vector db when a new page is added: just restart server\n‚úì Make clickable links instead of static sources (best value added).\n‚úì Try a few local models 3B to 8B\nFuture road map ü§î Maybe‚Ä¶\nStream response instead of showing all at the end.\nhttps://docs.streamlit.io/develop/api-reference/write-magic/st.write_stream\nCache - retrieve responses for similar question;\nhttps://docs.langchain.com/oss/python/integrations/text_embedding#caching\nRate limit, prevent ddos - check cloudflare settins?\nMemory context - include conversation history in the current querry\nsummarize context after length reaches LLM context limit\nDecision nodes - do a web search (can be a node) if rag semantic find returns empty\nPreload LLM: ollama run qwen2.5-coder:7b-instruct pros and cons?\nRotate thinking messages as the user waits\nStop button to cancel current thinking\nCore Components of the RAG application ‚úì Document loader All my posts are markdown files so for this step the tool of choice is:\nUnstructuredMarkdownLoader (works one file at a time).\nhttps://reference.langchain.com/v0.3/python/community/document_loaders/langchain_community.document_loaders.markdown.UnstructuredMarkdownLoader.html#langchain_community.document_loaders.markdown.UnstructuredMarkdownLoader.load\nUse DirectoryLoader to load all markdown files. Set UnstructuredMarkdownLoader to loader_cls parameter\nhttps://reference.langchain.com/v0.3/python/community/document_loaders/langchain_community.document_loaders.directory.DirectoryLoader.html#directoryloader\n‚úì Text splitter Split loaded documents using MarkdownTextSplitter\nhttps://reference.langchain.com/python/langchain_text_splitters/\n‚úì Embeddings Using sentence-transformers/all-MiniLM-L12-v2 for vectors embeddings and semantic search. Quick compare most downloaded models from huggingface.\nhttps://huggingface.co/models?other=text-embeddings-inference\u0026sort=downloads\nResearch on Embedding model choice did not point to an ideal model for my hardware but this will do for now.\n‚úì Vector Store Compared on FAISS / Chroma / InMemoryVectorStore Vector store choice\nBecause it has built-in persistence to disk i chose Chroma DB for my vector store.\nhttps://docs.langchain.com/oss/python/integrations/vectorstores/chroma\nAPI\nhttps://reference.langchain.com/python/integrations/langchain_chroma/\nAfter running a few tests with chroma db on the blog posts:\n-Regular search is working but often returns duplicates.\n-Using similarity_search_with_relevance_scores makes it difficult for the user to find anything with relevance higher than 0.3.\n‚ÄúNo relevant docs were retrieved using the relevance score threshold x‚Äù\n-Using similarity_search_with_score returns duplicates and score so i can at least return one relevant result or empty.\n-Using max_marginal_relevance_search diversifies the results but can return irrelevant docs.\n-Tweaking chunk size is difficult. User querry length has a big influence on score.\ngot Score: 0.46 for 1800 chunk size with 500 overlap.\ngot Score: 0.23 for 500, 200 chunk with 20 overlap\n-Split separators make big difference.\ngot 0.6 similarity score after adding specific markdown syntax from my blog posts (\"‚Äî\", ‚Äú##‚Äù, ‚Ä¶)\nchunk size 1000 or 500, overlap 100 or 0 made no difference\nchunk size 250 returns score 0.2 and misses the relevant chunk wher the search phrase is located\n‚úì Retriever Finding matching results in the vector db. Tested mostly with .2 similarity, returning max 5 text chunks. https://docs.langchain.com/oss/python/integrations/vectorstores/chroma#query-by-turning-into-retriever\n‚úì Prompt template Using ChatPromptTemplate to compose the prompt for the LLM:\nhttps://reference.langchain.com/python/langchain_core/prompts/\nExample:\nprompt = ChatPromptTemplate.from_messages([ (\"system\", \"You are a helpful assistant. Answer using the provided context.\" \"If the answer is not in the context, say 'Could not find content related to your query'.\"), (\"user\", \"Context:\\n{context}\\n\\nQuestion: {question}\") ]) ‚úì LLM model query Combining retrieved chunks with the user query into a special prompt which will be sent to a local LLM chat.\nChatOllama\nhttps://docs.langchain.com/oss/python/integrations/chat/ollama\nAPI langchain-ollama\nhttps://reference.langchain.com/python/integrations/langchain_ollama/\nOllama app MacOS install\nhttps://github.com/ollama/ollama?tab=readme-ov-file#ollama\nOllama app API reference\nhttps://docs.ollama.com/api/introduction\nOllama Python lib\nhttps://github.com/ollama/ollama-python\nAI models recommendation is all over the place, cant be sure what is real or outdated. I had to search directly in huggingface using filters and sorting.\nLLM Filter Checklist when browsing Hugging Face for this chat blog RAG app:\nIs it GGUF? ‚Üí Yes (CPU) Is it Instruct / Chat / IT? ‚Üí Yes (follows instructions) Is it ‚â§7B at Q4? ‚Üí Yes (fits RAM + speed) Does it say RAG? ‚Üí Bonus (Pleias only) Does Qwen/Llama/Mistral/Phi appear? ‚Üí Safe choices Is Q4_K_M or Q5_K_M in filename? ‚Üí Ideal\nExample high match find: pleias-rag-1b-instruct-q4_k_m.gguf\nExample good find: qwen2.5-7b-instruct-q4_k_m.gguf ‚úì UI for the RAG app Fast ui web app option for mvp testing in the browser: Streamlit Ui\nHomepage: streamlit.io\nDocs for chat message UI\nDeploy to Proxmox Moving the setup from mac arm to proxmox was probably the hardest part of the setup.\n‚úì Configure + Proxmox LXC Ollama Helper\nRun script to install Ollama LXC; Download LLM; start LLM\n# Pull and run the LLM model ollama pull qwen2.5-coder:7b-instruct \u0026\u0026 ollama run qwen2.5-coder:7b-instruct Of course, RAM memory limit error‚Ä¶ Increased Ollama LXC RAM to 8Gb,\nNope, did 16Gb increase (lost patience at this point), after attempting to install a second LLM: llama3.1:8b\n‚úì Create Debian LXC container using Proxmox helper script\nhttps://community-scripts.github.io/ProxmoxVE/scripts?id=debian\nSettings for ragblog web app:\nAdvanced install details\n-\u003e unpriviledged container type; Set pwd; Id 103; Hostname ragblog; Disk size 32; CPU cores 2; RAM 2048; Network bridge VMBR0; DHCP auto; DNS 8.8.8.8;\nPaste MAC OS pub key: cat ~/.ssh/id_ed*****.pub \u003e inside ssh step; Enable root ssh access; FUSE support, Tun/Tap no; Nesting Yes (causes wierd 256 warnings for Debian); GPU passthrough no; Keyctl support no; Container Protection Yes; device node creation no; filesystem mounts no; Enable verbose mode Yes;\nCopy RAG app to web app container.\nCommands to copy project folder: **scp** -r ./rag_blog_pages/ root@:/home/rag_blog Better for updates: **rsync** -avz ./rag_blog_pages/ root@:/home/rag_blog Install python dependencies.\n# activate local folder python env source .venv/bin/activate # check uv --version # Recreated .venv for x86 because in my case the folder was copied from a M4 ARM host rm -rf .venv uv venv # finally install uv add -r requirements.txt No space left on device error\npct stop 103 \u0026\u0026 pct resize 103 rootfs 32G \u0026\u0026 pct start 103 Finally start the damn thing (You can tell I‚Äôm loosing my calm at this point).\n... Local URL: http://localhost:8501 Network URL: http://: # What is this? whois indicates its my ISP providers ip?! External URL: http://: *Streamlit auto-detects your public IP by querying an external service and displays it as a convenience ‚Äî it‚Äôs just informational, not an actual open port.\nTo stop this behaviour:\n# .streamlit/config.toml [server] headless = true Shite, more changes.\nThe ollama server needs to be updated. Check correct IP is given to rag app after Ollama starts.\nollama_url: str = \"http://:\", Gateway proxy Install Cloudflared Proxmox helper script:\nhttps://community-scripts.github.io/ProxmoxVE/scripts?id=cloudflared\nSetup using Cloudflare settings\nEnsure the chat web app service restarts Avoid manually starting every time there is a problem or the container restarts.\nCreate the service file in the LXC container:\nvi /etc/systemd/system/rag_blog.service\n[Unit] Description=RAG Blog Streamlit App After=network.target [Service] Type=simple User= WorkingDirectory=/your_folder/rag_blog ExecStart=/your_folder/rag_blog/.venv/bin/python3 -m streamlit run chat_ui.py Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target Enable and restart. Run this in LXC container terminal:\nsystemctl daemon-reload systemctl enable rag_blog systemctl start rag_blog Restart container and check\nsystemctl status rag_blog Check logs from chat app\njournalctl -u rag_blog -f -o short-iso Clarifications for unknowns when starting this chat bot project\nOllama container\n-can it run in LXC container? YES but has limitations\nCloudflared container:\n-can this live in a lxc container? YES\n-how can the RAG app make requests? RAG to LLM container api. In your RAG app, use OpenAI-compatible clients (expose OpenAI-compatible APIs)\nA small test was run on mac mini to check connection.\n# In your RAG app from langchain_community.llms import Ollama llm = Ollama( base_url=\"http://ollama:11434\", model=\"llama3.2\" # or mistral, phi, etc. ) Below is just a visual reference using a Docker compose file, to help understand RAG-Ollama container Network (not including cloudflared).\n# docker-compose.yml services: ollama: image: ollama/ollama ports: - \"11434:11434\" volumes: - ollama:/root/.ollama networks: - rag-network rag-app: build: . networks: - rag-network environment: - OLLAMA_BASE_URL=http://ollama:11434 volumes: ollama: networks: rag-network: Problems and solutions to RAG search ‚ö†Ô∏è Weak response (i dont know) even if relevant chunks are returned.\n‚úÖ Solution: Switch to a slightly larger model. Eg. from q4 to q5 (llama3-chatqa:8b-v1.5-q4_K_M -\u003e llama3-chatqa:8b-v1.5-q5_K_M).\n‚ö†Ô∏è No results are returned by very low threshold of 2 for simple questions like: What topics are covered?; Who is the author?\nCould be a vector embeddings, semantic search issue related to threshold, because no results are returned even for low score of 1 but related content does exist on the blog. ‚ö†Ô∏è Answer is very slow.\nPossibly due to local small LLM, or slow hardware. ATM only idea is provide subscription LLM API. ‚ö†Ô∏è LLM do not follow instructions well. Does not mention of missing / not found context. (when using qwen2.5-coder:7b-instruct)\n‚úÖ Solution:\nTry different LLM models with instruction training. llama3-chatqa:8b-v1.5-q4_K_M seems a better for this particular rag app.\n‚ö†Ô∏è LLM answers even if no posts were found by the semantic search.\n‚úÖ Solution:\nPrevent llm run with if block if no chunks are returned by semantic search.\n‚ö†Ô∏è LXC container memory is full and no more models can be pulled\n‚úÖ Solution:\nrm -rf /root/.ollama/models/blobs/* rm -rf /root/.ollama/models/manifests/* ‚ö†Ô∏è Icons from chat revert to Streamlit default after a code change followed by a page refresh\n‚úÖ Solution:\n# Avatar must be appended with chat message and returned for display st.session_state.messages.append({ \"role\": \"user\", \"content\": prompt, \"avatar\": \"üë§\", }) # Display chat messages for msg in st.session_state.messages: with st.chat_message(msg[\"role\"], avatar=msg.get(\"avatar\")): ... ‚ö†Ô∏è About me is not considered/found: Semantic search is completly missing the about blog summary document\n‚úÖ Solution:\nChanging the embedding model from sentence-transformers/all-MiniLM-L6-v2 to sentence-transformers/all-MiniLM-L12-v2\nEg. for query ‚Äúgive me a summary of this blog and about liviu iancu‚Äù\nBefore: poor semantic search results, containting word ‚Äúabout‚Äù but 0.3 relevance match.\nsentence-transformers/all-MiniLM-L6-v2 After: changing model, got 0.6 relevance match.\nsentence-transformers/all-MiniLM-L12-v2 Thoughts about the RAG chat This RAG chat app was a hefty challenge to do on my hardware, and get any decent results.\nThere were many other details about setting up cloudflared and python setup that i have left out of this post because its long enough already.\nCurent advantage: no api cost overflow risk. Disadvantage: slow on my hardware, 6 - 20 seconds response.\nI may consider using a paid, limited API in the future, however the proof of concept works. For future project i have a bunch of more serious use cases and painpoints to solve for myself with different chat apps so this project will not be updated.\nHoping this will be useful to you if you attempt to build something similar.\nUntil next time, have fun building somethin new!\n","wordCount":"1921","inLanguage":"en","datePublished":"2026-02-26T00:00:00Z","dateModified":"2026-02-26T00:00:00Z","author":{"@type":"Person","name":"Liviu Iancu"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://liviuiancu.com/posts/chat-blog-rag-app/"},"publisher":{"@type":"Organization","name":"Dev Pages","logo":{"@type":"ImageObject","url":"https://liviuiancu.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://liviuiancu.com/ accesskey=h title="Dev Pages (Alt + H)">Dev Pages</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://liviuiancu.com/archives title=Archive><span>Archive</span></a></li><li><a href=https://liviuiancu.com/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://liviuiancu.com/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://liviuiancu.com/>Home</a>&nbsp;¬ª&nbsp;<a href=https://liviuiancu.com/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Blog chat based on RAG AI with local small LLM</h1><div class=post-meta><span title='2026-02-26 00:00:00 +0000 UTC'>February 26, 2026</span>&nbsp;¬∑&nbsp;10 min&nbsp;¬∑&nbsp;Liviu Iancu</div></header><div class=post-content><p>Providing a RAG (Retrieval-Augmented Generation) AI for my blog.<br>The goal: a privacy-first, locally hosted assistant that only answers based on my posts. No external APIs, no subscriptions ‚Äî just my hardware and local models.</p><p>Below are steps I went through in the building process, requiremets, whishlist roadmap, errors and proposed solutions. This article was built more like a reference than a story so feel free to jump where you need.</p><h3 id=table-of-contents>Table of Contents<a hidden class=anchor aria-hidden=true href=#table-of-contents>#</a></h3><ul><li><a href=/posts/chat-blog-rag-app/#the-tech-stack>The Tech Stack</a></li><li><a href=/posts/chat-blog-rag-app/#architecture-&-query-flow>Architecture & Query Flow</a></li><li><a href=/posts/chat-blog-rag-app/#implemented-rag-features>Implemented RAG Features</a></li><li><a href=/posts/chat-blog-rag-app/#future-roadmap>Future Roadmap</a></li><li><a href=/posts/chat-blog-rag-app/#core-components-o-the-rag-application>Core Components</a><ul><li><a href=/posts/chat-blog-rag-app/#document-loader>Document loader</a></li><li><a href=/posts/chat-blog-rag-app/#text-splitter>Text splitter</a></li><li><a href=/posts/chat-blog-rag-app/#embeddigns>Embeddings</a></li><li><a href=/posts/chat-blog-rag-app/#vector-store>Vector Store</a></li><li><a href=/posts/chat-blog-rag-app/#retriever>Retriever</a></li><li><a href=/posts/chat-blog-rag-app/#prompt-template>Prompt template</a></li><li><a href=/posts/chat-blog-rag-app/#llm-model-query>LLM model query</a></li><li><a href=/posts/chat-blog-rag-app/#ui-for-the-rag-app>UI for the RAG app</a></li></ul></li><li><a href=/posts/chat-blog-rag-app/#deploy-to-proxmox>Deployment to Proxmox</a></li><li><a href=/posts/chat-blog-rag-app/#problems-and-solutions-to-rag-search>Problems and solutions to RAG search</a></li><li><a href=/posts/chat-blog-rag-app/#thoughts-about-the-rag-chat>Thoughts about the RAG chat</a></li></ul><h3 id=the-tech-stack>The Tech Stack<a hidden class=anchor aria-hidden=true href=#the-tech-stack>#</a></h3><table><thead><tr><th style=text-align:left>Component</th><th style=text-align:left>Technology</th></tr></thead><tbody><tr><td style=text-align:left><strong>UI</strong></td><td style=text-align:left><a href=https://streamlit.io/>Streamlit</a></td></tr><tr><td style=text-align:left><strong>Orchestration</strong></td><td style=text-align:left><a href=https://www.langchain.com/>LangChain</a></td></tr><tr><td style=text-align:left><strong>Vector Store</strong></td><td style=text-align:left><a href=https://docs.trychroma.com/>Chroma DB</a> (Local disk persistence)</td></tr><tr><td style=text-align:left><strong>Embeddings</strong></td><td style=text-align:left>Local small embeddings model</td></tr><tr><td style=text-align:left><strong>LLM</strong></td><td style=text-align:left>Local model on <a href=https://ollama.com/>Ollama</a></td></tr></tbody></table><h3 id=architecture--query-flow>Architecture & Query Flow<a hidden class=anchor aria-hidden=true href=#architecture--query-flow>#</a></h3><p>The request travels from blog to the local LLM<br><code>Blog -> Cloudflare URL -> cloudflared -> RAG Container (UI) -> LLM Container (Ollama)</code><br>Integration with blog as a simple link on first page or separate page containing link to a local running container.</p><h2 id=implemented-rag-features>Implemented RAG Features<a hidden class=anchor aria-hidden=true href=#implemented-rag-features>#</a></h2><p>‚úì Only answer based on blog posts. Say i cant find relevant knowledge if no context found<br>‚úì Should work local, no remote calls or subscription required<br>‚úì Be as simple as possible in the beginning.<br>‚úì Citations - point to source file<br>‚úì Vector store: store vectors embeddings from text in a local memory database<br>‚úì Build a simple user interface using Streamlit UI.<br>‚úì Update vector db when a new page is added: just restart server<br>‚úì Make clickable links instead of static sources (best value added).<br>‚úì Try a few local models 3B to 8B</p><h3 id=future-road-map>Future road map<a hidden class=anchor aria-hidden=true href=#future-road-map>#</a></h3><p>ü§î Maybe&mldr;<br>Stream response instead of showing all at the end.<br><a href=https://docs.streamlit.io/develop/api-reference/write-magic/st.write_stream>https://docs.streamlit.io/develop/api-reference/write-magic/st.write_stream</a><br>Cache - retrieve responses for similar question;<br><a href=https://docs.langchain.com/oss/python/integrations/text_embedding#caching>https://docs.langchain.com/oss/python/integrations/text_embedding#caching</a><br>Rate limit, prevent ddos - check cloudflare settins?<br>Memory context - include conversation history in the current querry<br>summarize context after length reaches LLM context limit<br>Decision nodes - do a web search (can be a node) if rag semantic find returns empty<br>Preload LLM: ollama run qwen2.5-coder:7b-instruct pros and cons?<br>Rotate thinking messages as the user waits<br>Stop button to cancel current thinking</p><h2 id=core-components-of-the-rag-application>Core Components of the RAG application<a hidden class=anchor aria-hidden=true href=#core-components-of-the-rag-application>#</a></h2><h2 id=-document-loader>‚úì Document loader<a hidden class=anchor aria-hidden=true href=#-document-loader>#</a></h2><p>All my posts are markdown files so for this step the tool of choice is:<br>UnstructuredMarkdownLoader (works one file at a time).<br><a href=https://reference.langchain.com/v0.3/python/community/document_loaders/langchain_community.document_loaders.markdown.UnstructuredMarkdownLoader.html#langchain_community.document_loaders.markdown.UnstructuredMarkdownLoader.load>https://reference.langchain.com/v0.3/python/community/document_loaders/langchain_community.document_loaders.markdown.UnstructuredMarkdownLoader.html#langchain_community.document_loaders.markdown.UnstructuredMarkdownLoader.load</a><br>Use DirectoryLoader to load all markdown files. Set UnstructuredMarkdownLoader to loader_cls parameter<br><a href=https://reference.langchain.com/v0.3/python/community/document_loaders/langchain_community.document_loaders.directory.DirectoryLoader.html#directoryloader>https://reference.langchain.com/v0.3/python/community/document_loaders/langchain_community.document_loaders.directory.DirectoryLoader.html#directoryloader</a></p><h2 id=-text-splitter>‚úì Text splitter<a hidden class=anchor aria-hidden=true href=#-text-splitter>#</a></h2><p>Split loaded documents using MarkdownTextSplitter<br><a href=https://reference.langchain.com/python/langchain_text_splitters/>https://reference.langchain.com/python/langchain_text_splitters/</a></p><h2 id=-embeddings>‚úì Embeddings<a hidden class=anchor aria-hidden=true href=#-embeddings>#</a></h2><p>Using sentence-transformers/all-MiniLM-L12-v2 for vectors embeddings and semantic search.
Quick compare most downloaded models from huggingface.<br><a href="https://huggingface.co/models?other=text-embeddings-inference&amp;sort=downloads">https://huggingface.co/models?other=text-embeddings-inference&amp;sort=downloads</a><br>Research on Embedding model choice did not point to an ideal model for my hardware but this will do for now.</p><h2 id=-vector-store>‚úì Vector Store<a hidden class=anchor aria-hidden=true href=#-vector-store>#</a></h2><p>Compared on FAISS / Chroma / InMemoryVectorStore Vector store choice<br>Because it has built-in persistence to disk i chose <strong>Chroma DB</strong> for my vector store.<br><a href=https://docs.langchain.com/oss/python/integrations/vectorstores/chroma>https://docs.langchain.com/oss/python/integrations/vectorstores/chroma</a><br>API<br><a href=https://reference.langchain.com/python/integrations/langchain_chroma/>https://reference.langchain.com/python/integrations/langchain_chroma/</a></p><p>After running a few tests with chroma db on the blog posts:<br>-Regular search is working but often returns duplicates.<br>-Using similarity_search_with_relevance_scores makes it difficult for the user to find anything with relevance higher than 0.3.<br>&ldquo;No relevant docs were retrieved using the relevance score threshold x&rdquo;<br>-Using similarity_search_with_score returns duplicates and score so i can at least return one relevant result or empty.<br>-Using max_marginal_relevance_search diversifies the results but can return irrelevant docs.<br>-Tweaking chunk size is difficult. User querry length has a big influence on score.<br>got Score: 0.46 for 1800 chunk size with 500 overlap.<br>got Score: 0.23 for 500, 200 chunk with 20 overlap<br>-Split separators make big difference.<br>got 0.6 similarity score after adding specific markdown syntax from my blog posts ("&mdash;", &ldquo;##&rdquo;, &mldr;)<br>chunk size 1000 or 500, overlap 100 or 0 made no difference<br>chunk size 250 returns score 0.2 and misses the relevant chunk wher the search phrase is located</p><h2 id=-retriever>‚úì Retriever<a hidden class=anchor aria-hidden=true href=#-retriever>#</a></h2><p>Finding matching results in the vector db. Tested mostly with .2 similarity, returning max 5 text chunks.
<a href=https://docs.langchain.com/oss/python/integrations/vectorstores/chroma#query-by-turning-into-retriever>https://docs.langchain.com/oss/python/integrations/vectorstores/chroma#query-by-turning-into-retriever</a></p><h2 id=-prompt-template>‚úì Prompt template<a hidden class=anchor aria-hidden=true href=#-prompt-template>#</a></h2><p>Using ChatPromptTemplate to compose the prompt for the LLM:<br><a href=https://reference.langchain.com/python/langchain_core/prompts/>https://reference.langchain.com/python/langchain_core/prompts/</a><br>Example:</p><pre tabindex=0><code>prompt = ChatPromptTemplate.from_messages([
    (&#34;system&#34;, &#34;You are a helpful assistant. Answer using the provided context.&#34;
        &#34;If the answer is not in the context, say &#39;Could not find content related to your query&#39;.&#34;),
    (&#34;user&#34;, &#34;Context:\n{context}\n\nQuestion: {question}&#34;)
])
</code></pre><h2 id=-llm-model-query>‚úì LLM model query<a hidden class=anchor aria-hidden=true href=#-llm-model-query>#</a></h2><p>Combining retrieved chunks with the user query into a special prompt which will be sent to a local LLM chat.<br>ChatOllama<br><a href=https://docs.langchain.com/oss/python/integrations/chat/ollama>https://docs.langchain.com/oss/python/integrations/chat/ollama</a><br>API langchain-ollama<br><a href=https://reference.langchain.com/python/integrations/langchain_ollama/>https://reference.langchain.com/python/integrations/langchain_ollama/</a><br>Ollama app MacOS install<br><a href="https://github.com/ollama/ollama?tab=readme-ov-file#ollama">https://github.com/ollama/ollama?tab=readme-ov-file#ollama</a><br>Ollama app API reference<br><a href=https://docs.ollama.com/api/introduction>https://docs.ollama.com/api/introduction</a><br>Ollama Python lib<br><a href=https://github.com/ollama/ollama-python>https://github.com/ollama/ollama-python</a><br>AI models recommendation is all over the place, cant be sure what is real or outdated. I had to search directly in huggingface using filters and sorting.</p><p>LLM Filter Checklist when browsing Hugging Face for this chat blog RAG app:</p><ol><li>Is it <strong>GGUF</strong>? ‚Üí Yes (CPU)</li><li>Is it <strong>Instruct / Chat / IT</strong>? ‚Üí Yes (follows instructions)</li><li>Is it ‚â§7B at Q4? ‚Üí Yes (fits RAM + speed)</li><li>Does it say RAG? ‚Üí Bonus (Pleias only)</li><li>Does Qwen/Llama/Mistral/Phi appear? ‚Üí Safe choices</li><li>Is Q4_K_M or Q5_K_M in filename? ‚Üí Ideal<br>Example high match find: pleias-rag-1b-instruct-q4_k_m.gguf<br>Example good find: qwen2.5-7b-instruct-q4_k_m.gguf</li></ol><h2 id=-ui-for-the-rag-app>‚úì UI for the RAG app<a hidden class=anchor aria-hidden=true href=#-ui-for-the-rag-app>#</a></h2><p>Fast ui web app option for mvp testing in the browser: Streamlit Ui<br>Homepage: <a href=https://streamlit.io/>streamlit.io</a><br><a href=https://docs.streamlit.io/develop/api-reference/chat/st.chat_message>Docs for chat message UI</a></p><h2 id=deploy-to-proxmox>Deploy to Proxmox<a hidden class=anchor aria-hidden=true href=#deploy-to-proxmox>#</a></h2><p>Moving the setup from mac arm to proxmox was probably the hardest part of the setup.</p><p>‚úì Configure + Proxmox LXC Ollama Helper<br>Run script to install Ollama LXC; Download LLM; start LLM</p><pre tabindex=0><code># Pull and run the LLM model
ollama pull qwen2.5-coder:7b-instruct &amp;&amp;
ollama run qwen2.5-coder:7b-instruct
</code></pre><p>Of course, RAM memory limit error&mldr; Increased Ollama LXC RAM to 8Gb,<br>Nope, did 16Gb increase (lost patience at this point), after attempting to install a second LLM: llama3.1:8b</p><p>‚úì Create Debian LXC container using Proxmox helper script<br><a href="https://community-scripts.github.io/ProxmoxVE/scripts?id=debian">https://community-scripts.github.io/ProxmoxVE/scripts?id=debian</a></p><p><strong>Settings for ragblog web app:</strong><br>Advanced install details<br>-> unpriviledged container type; Set pwd; Id 103; Hostname ragblog; Disk size 32; CPU cores 2; RAM 2048; Network bridge VMBR0; DHCP auto; DNS 8.8.8.8;<br>Paste MAC OS pub key: cat ~/.ssh/id_ed*****.pub > inside ssh step; Enable root ssh access; FUSE support, Tun/Tap no; Nesting Yes (causes wierd 256 warnings for Debian); GPU passthrough no; Keyctl support no; Container Protection Yes; device node creation no; filesystem mounts no; Enable verbose mode Yes;</p><p>Copy RAG app to web app container.</p><pre tabindex=0><code>Commands to copy project folder:  
  **scp** -r ./rag_blog_pages/ root@&lt;chat_app_ip&gt;:/home/rag_blog  
  Better for updates:  
  **rsync** -avz ./rag_blog_pages/ root@&lt;chat_app_ip&gt;:/home/rag_blog  
</code></pre><p>Install python dependencies.</p><pre tabindex=0><code># activate local folder python env
source .venv/bin/activate

# check
uv --version
</code></pre><pre tabindex=0><code># Recreated .venv for x86 because in my case the folder was copied from a M4 ARM host
rm -rf .venv 
uv venv

# finally install
uv add -r requirements.txt
</code></pre><p>No space left on device error</p><pre tabindex=0><code>pct stop 103 &amp;&amp;
pct resize 103 rootfs 32G &amp;&amp;
pct start 103
</code></pre><p>Finally start the damn thing (You can tell I&rsquo;m loosing my calm at this point).</p><pre tabindex=0><code>...
Local URL: http://localhost:8501
Network URL: http://&lt;chat_webapp_ip&gt;:&lt;port&gt;

# What is this? whois indicates its my ISP providers ip?!
External URL: http://&lt;my_ip&gt;:&lt;ollama_ip&gt;
</code></pre><p>*Streamlit auto-detects your public IP by querying an external service and displays it as a convenience ‚Äî it&rsquo;s just informational, not an actual open port.<br>To stop this behaviour:</p><pre tabindex=0><code># .streamlit/config.toml
[server]
headless = true
</code></pre><p>Shite, more changes.<br>The ollama server needs to be updated. Check correct IP is given to rag app after Ollama starts.</p><pre tabindex=0><code>ollama_url: str = &#34;http://&lt;ollama_ip&gt;:&lt;port&gt;&#34;,
</code></pre><h2 id=gateway-proxy>Gateway proxy<a hidden class=anchor aria-hidden=true href=#gateway-proxy>#</a></h2><p>Install Cloudflared Proxmox helper script:<br><a href="https://community-scripts.github.io/ProxmoxVE/scripts?id=cloudflared">https://community-scripts.github.io/ProxmoxVE/scripts?id=cloudflared</a><br>Setup using Cloudflare settings</p><h2 id=ensure-the-chat-web-app-service-restarts>Ensure the chat web app service restarts<a hidden class=anchor aria-hidden=true href=#ensure-the-chat-web-app-service-restarts>#</a></h2><p>Avoid manually starting every time there is a problem or the container restarts.</p><p>Create the service file in the LXC container:<br>vi /etc/systemd/system/rag_blog.service</p><pre tabindex=0><code>[Unit]
Description=RAG Blog Streamlit App
After=network.target

[Service]
Type=simple
User=&lt;default_container_user&gt;
WorkingDirectory=/your_folder/rag_blog
ExecStart=/your_folder/rag_blog/.venv/bin/python3 -m streamlit run chat_ui.py
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
</code></pre><p>Enable and restart. Run this in LXC container terminal:</p><pre tabindex=0><code>systemctl daemon-reload
systemctl enable rag_blog
systemctl start rag_blog
</code></pre><p>Restart container and check</p><pre tabindex=0><code>systemctl status rag_blog
</code></pre><p>Check logs from chat app</p><pre tabindex=0><code>journalctl -u rag_blog -f -o short-iso
</code></pre><p><strong>Clarifications for unknowns when starting this chat bot project</strong></p><p>Ollama container<br>-can it run in LXC container? YES but has limitations</p><p>Cloudflared container:<br>-can this live in a lxc container? YES<br>-how can the RAG app make requests?
RAG to LLM container api.
In your RAG app, use OpenAI-compatible clients (expose OpenAI-compatible APIs)</p><p>A small test was run on mac mini to check connection.</p><pre tabindex=0><code># In your RAG app
from langchain_community.llms import Ollama

llm = Ollama(
    base_url=&#34;http://ollama:11434&#34;,
    model=&#34;llama3.2&#34;  # or mistral, phi, etc.
)
</code></pre><p>Below is just a visual reference using a Docker compose file, to help understand RAG-Ollama container Network (not including cloudflared).</p><pre tabindex=0><code># docker-compose.yml
services:
  ollama:
    image: ollama/ollama
    ports:
      - &#34;11434:11434&#34;
    volumes:
      - ollama:/root/.ollama
    networks:
      - rag-network
  
  rag-app:
    build: .
    networks:
      - rag-network
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434

volumes:
  ollama:
networks:
  rag-network:
</code></pre><h2 id=problems-and-solutions-to-rag-search>Problems and solutions to RAG search<a hidden class=anchor aria-hidden=true href=#problems-and-solutions-to-rag-search>#</a></h2><p>‚ö†Ô∏è Weak response (i dont know) even if relevant chunks are returned.<br>‚úÖ Solution: Switch to a slightly larger model. Eg. from q4 to q5 (llama3-chatqa:8b-v1.5-q4_K_M -> llama3-chatqa:8b-v1.5-q5_K_M).</p><p>‚ö†Ô∏è No results are returned by very low threshold of 2 for simple questions like: What topics are covered?; Who is the author?</p><ul><li>Could be a vector embeddings, semantic search issue related to threshold, because no results are returned even for low score of 1 but related content does exist on the blog.</li></ul><p>‚ö†Ô∏è Answer is very slow.</p><ul><li>Possibly due to local small LLM, or slow hardware. ATM only idea is provide subscription LLM API.</li></ul><p>‚ö†Ô∏è LLM do not follow instructions well. Does not mention of missing / not found context. (when using qwen2.5-coder:7b-instruct)<br>‚úÖ Solution:<br>Try different LLM models with instruction training. llama3-chatqa:8b-v1.5-q4_K_M seems a better for this particular rag app.</p><p>‚ö†Ô∏è LLM answers even if no posts were found by the semantic search.<br>‚úÖ Solution:<br>Prevent llm run with if block if no chunks are returned by semantic search.</p><p>‚ö†Ô∏è LXC container memory is full and no more models can be pulled<br>‚úÖ Solution:</p><pre tabindex=0><code>rm -rf /root/.ollama/models/blobs/*
rm -rf /root/.ollama/models/manifests/*
</code></pre><p>‚ö†Ô∏è Icons from chat revert to Streamlit default after a code change followed by a page refresh<br>‚úÖ Solution:</p><pre tabindex=0><code># Avatar must be appended with chat message and returned for display
    st.session_state.messages.append({
        &#34;role&#34;: &#34;user&#34;,
        &#34;content&#34;: prompt,
        &#34;avatar&#34;: &#34;üë§&#34;,
    })
# Display chat messages
for msg in st.session_state.messages:
    with st.chat_message(msg[&#34;role&#34;], avatar=msg.get(&#34;avatar&#34;)):
...
</code></pre><p>‚ö†Ô∏è About me is not considered/found: Semantic search is completly missing the about blog summary document<br>‚úÖ Solution:<br>Changing the embedding model from sentence-transformers/all-MiniLM-L6-v2 to sentence-transformers/all-MiniLM-L12-v2<br>Eg. for query &ldquo;give me a summary of this blog and about liviu iancu&rdquo;</p><p><strong>Before:</strong> poor semantic search results, containting word &ldquo;about&rdquo; but 0.3 relevance match.</p><pre tabindex=0><code>sentence-transformers/all-MiniLM-L6-v2 
</code></pre><p><strong>After:</strong> changing model, got 0.6 relevance match.</p><pre tabindex=0><code>sentence-transformers/all-MiniLM-L12-v2
</code></pre><h2 id=thoughts-about-the-rag-chat>Thoughts about the RAG chat<a hidden class=anchor aria-hidden=true href=#thoughts-about-the-rag-chat>#</a></h2><p>This RAG chat app was a hefty challenge to do on my hardware, and get any decent results.<br>There were many other details about setting up cloudflared and python setup that i have left out of this post because its long enough already.</p><p>Curent advantage: no api cost overflow risk.
Disadvantage: slow on my hardware, 6 - 20 seconds response.</p><p>I may consider using a paid, limited API in the future, however the proof of concept works. For future project i have a bunch of more serious use cases and painpoints to solve for myself with different chat apps so this project will not be updated.</p><p>Hoping this will be useful to you if you attempt to build something similar.<br>Until next time, have fun building somethin new!</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://liviuiancu.com/tags/rag-app/>Rag App</a></li><li><a href=https://liviuiancu.com/tags/ai-app/>Ai App</a></li><li><a href=https://liviuiancu.com/tags/chat-bot/>Chat Bot</a></li><li><a href=https://liviuiancu.com/tags/documents-chat/>Documents Chat</a></li></ul><nav class=paginav><a class=next href=https://liviuiancu.com/posts/proxmox-for-dokploy/><span class=title>Next ¬ª</span><br><span>Running Proxmox and Dokploy for my home lab</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Blog chat based on RAG AI with local small LLM on x" href="https://x.com/intent/tweet/?text=Blog%20chat%20based%20on%20RAG%20AI%20with%20local%20small%20LLM&amp;url=https%3a%2f%2fliviuiancu.com%2fposts%2fchat-blog-rag-app%2f&amp;hashtags=ragapp%2caiapp%2cchatbot%2cdocumentschat"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Blog chat based on RAG AI with local small LLM on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fliviuiancu.com%2fposts%2fchat-blog-rag-app%2f&amp;title=Blog%20chat%20based%20on%20RAG%20AI%20with%20local%20small%20LLM&amp;summary=Blog%20chat%20based%20on%20RAG%20AI%20with%20local%20small%20LLM&amp;source=https%3a%2f%2fliviuiancu.com%2fposts%2fchat-blog-rag-app%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Blog chat based on RAG AI with local small LLM on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fliviuiancu.com%2fposts%2fchat-blog-rag-app%2f&title=Blog%20chat%20based%20on%20RAG%20AI%20with%20local%20small%20LLM"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Blog chat based on RAG AI with local small LLM on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fliviuiancu.com%2fposts%2fchat-blog-rag-app%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Blog chat based on RAG AI with local small LLM on whatsapp" href="https://api.whatsapp.com/send?text=Blog%20chat%20based%20on%20RAG%20AI%20with%20local%20small%20LLM%20-%20https%3a%2f%2fliviuiancu.com%2fposts%2fchat-blog-rag-app%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Blog chat based on RAG AI with local small LLM on telegram" href="https://telegram.me/share/url?text=Blog%20chat%20based%20on%20RAG%20AI%20with%20local%20small%20LLM&amp;url=https%3a%2f%2fliviuiancu.com%2fposts%2fchat-blog-rag-app%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Blog chat based on RAG AI with local small LLM on ycombinator" href="https://news.ycombinator.com/submitlink?t=Blog%20chat%20based%20on%20RAG%20AI%20with%20local%20small%20LLM&u=https%3a%2f%2fliviuiancu.com%2fposts%2fchat-blog-rag-app%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>¬© <a href=https://github.com/adityatelange/hugo-PaperMod/graphs/contributors>PaperMod Contributors</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>